{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "import random\n",
    "import re\n",
    "# import scrapy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# reading the web page and print the status code\n",
    "r = requests.get(url)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars0.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars1.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars2.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars3.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazona\n"
     ]
    }
   ],
   "source": [
    "# print the response text\n",
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the HTML using beautifulsoup\n",
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = soup.find('div', class_='col-md-6').text\n",
    "# name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rob Dodson'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# find a single name\n",
    "name = soup.find('h1', class_='h3').text.replace('\\n','').strip()\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robdodson'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find a single nickname\n",
    "nickname = soup.find('p', class_='f4 text-normal mb-1').text.replace('\\n','').replace(' ','')\n",
    "nickname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rob Dodson',\n",
       " 'MichaIng',\n",
       " 'Gleb Bahmutov',\n",
       " 'Lukas Taegert-Atkinson',\n",
       " 'Till Krüss',\n",
       " 'Jesse Duffield',\n",
       " 'ᴜɴᴋɴᴡᴏɴ',\n",
       " 'Arve Knudsen',\n",
       " 'Niklas von Hertzen',\n",
       " 'Stephen Celis',\n",
       " 'Damian Dulisz',\n",
       " 'Yufan You',\n",
       " 'Christian Clauss',\n",
       " 'Jirka Borovec',\n",
       " 'Timothy Edmund Crosley',\n",
       " 'James Newton-King',\n",
       " 'Michael Shilman',\n",
       " 'Mike Penz',\n",
       " 'Alex Hall',\n",
       " 'Diego Sampaio',\n",
       " 'Dries Vints',\n",
       " 'JK Jung',\n",
       " 'Steven',\n",
       " 'Daniel Martí',\n",
       " 'Łukasz Magiera']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all the names & nicknames\n",
    "\n",
    "# for name in soup.find_all('h1', class_='h3'):\n",
    "#     print(name.text.replace('\\n','').strip())\n",
    "\n",
    "# list comprehension NAMES\n",
    "names = [name.text.replace('\\n','').strip() for name in soup.find_all('h1', class_='h3')]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robdodson',\n",
       " 'bahmutov',\n",
       " 'lukastaegert',\n",
       " 'tillkruss',\n",
       " 'jesseduffield',\n",
       " 'unknwon',\n",
       " 'aknuds1',\n",
       " 'niklasvh',\n",
       " 'stephencelis',\n",
       " 'shentao',\n",
       " 'ouuan',\n",
       " 'cclauss',\n",
       " 'Borda',\n",
       " 'timothycrosley',\n",
       " 'JamesNK',\n",
       " 'shilman',\n",
       " 'mikepenz',\n",
       " 'alexmojaki',\n",
       " 'sampaiodiego',\n",
       " 'driesvints',\n",
       " 'jkjung-avt',\n",
       " 'styfle',\n",
       " 'mvdan',\n",
       " 'magik6k']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehension NICKNAMES\n",
    "nicknames = [nickname.text.replace('\\n','').replace(' ','') for nickname in soup.find_all('p', class_='f4 text-normal mb-1')]\n",
    "nicknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rob Dodson (robdodson)', 'MichaIng (bahmutov)', 'Gleb Bahmutov (lukastaegert)', 'Lukas Taegert-Atkinson (tillkruss)', 'Till Krüss (jesseduffield)', 'Jesse Duffield (unknwon)', 'ᴜɴᴋɴᴡᴏɴ (aknuds1)', 'Arve Knudsen (niklasvh)', 'Niklas von Hertzen (stephencelis)', 'Stephen Celis (shentao)', 'Damian Dulisz (ouuan)', 'Yufan You (cclauss)', 'Christian Clauss (Borda)', 'Jirka Borovec (timothycrosley)', 'Timothy Edmund Crosley (JamesNK)', 'James Newton-King (shilman)', 'Michael Shilman (mikepenz)', 'Mike Penz (alexmojaki)', 'Alex Hall (sampaiodiego)', 'Diego Sampaio (driesvints)', 'Dries Vints (jkjung-avt)', 'JK Jung (styfle)', 'Steven (mvdan)', 'Daniel Martí (magik6k)']\n"
     ]
    }
   ],
   "source": [
    "# final list of names \n",
    "list_names= []\n",
    "for name, nickname in zip(names, nicknames):\n",
    "    i = name + ' (' + nickname +')'\n",
    "    # print(f'{name} ({nickname})')\n",
    "    list_names.append(i)\n",
    "print(list_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url1 = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(url1)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars0.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars1.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars2.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars3.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazona\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/executablebooks/jupyter-book'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find a single repo\n",
    "repo = soup.find('h1', class_='h3 lh-condensed').find('a')['href'].replace(r\"^\\W+\", \"\")\n",
    "repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/executablebooks/jupyter-book',\n",
       " '/sherlock-project/sherlock',\n",
       " '/cvg/Hierarchical-Localization',\n",
       " '/PrefectHQ/prefect',\n",
       " '/RUB-SysSec/mobile_sentinel',\n",
       " '/naiveHobo/InvoiceNet',\n",
       " '/iswbm/magic-python',\n",
       " '/google-research/bert',\n",
       " '/geerlingguy/ansible-for-devops',\n",
       " '/ansible/ansible',\n",
       " '/aio-libs/aiohttp',\n",
       " '/pythonstock/stock',\n",
       " '/rusty1s/pytorch_geometric',\n",
       " '/ekzhang/fastseg',\n",
       " '/mks0601/I2L-MeshNet_RELEASE',\n",
       " '/microsoft/playwright-python',\n",
       " '/d2l-ai/d2l-en',\n",
       " '/clovaai/stargan-v2',\n",
       " '/shibing624/pycorrector',\n",
       " '/huggingface/transformers',\n",
       " '/yangjianxin1/GPT2-chitchat',\n",
       " '/UKPLab/sentence-transformers',\n",
       " '/stanfordnlp/stanza',\n",
       " '/secdev/scapy',\n",
       " '/google/diff-match-patch']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehension REPOS\n",
    "repos = [repo.find('a')['href'] for repo in soup.find_all('h1', class_='h3 lh-condensed')]\n",
    "repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['executablebooks/jupyter-book', 'sherlock-project/sherlock', 'cvg/Hierarchical-Localization', 'PrefectHQ/prefect', 'RUB-SysSec/mobile_sentinel', 'naiveHobo/InvoiceNet', 'iswbm/magic-python', 'google-research/bert', 'geerlingguy/ansible-for-devops', 'ansible/ansible', 'aio-libs/aiohttp', 'pythonstock/stock', 'rusty1s/pytorch_geometric', 'ekzhang/fastseg', 'mks0601/I2L-MeshNet_RELEASE', 'microsoft/playwright-python', 'd2l-ai/d2l-en', 'clovaai/stargan-v2', 'shibing624/pycorrector', 'huggingface/transformers', 'yangjianxin1/GPT2-chitchat', 'UKPLab/sentence-transformers', 'stanfordnlp/stanza', 'secdev/scapy', 'google/diff-match-patch']\n"
     ]
    }
   ],
   "source": [
    "# final list of repos\n",
    "list_repos= []\n",
    "for i in repos:\n",
    "    i = re.sub(r\"^\\W+\", \"\", i)\n",
    "    list_repos.append(i)\n",
    "print(list_repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "r = requests.get(url2)\n",
    "r.status_code    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Walt Disney - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"297a02b6-04a9-4477-ba78-b88b7c6963c2\",\"wgCSP\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Walt_Disney_1946.JPG'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find a table image\n",
    "img_table = soup.find('table', class_='infobox biography vcard').find('a')['href'].replace(r'/wiki/File:','')\n",
    "img_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imm = soup.find('a', attrs={'class':'image'}).find('img')['alt']\n",
    "# imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = soup.find('a', attrs={'class':'image'}).find('img')['src']\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehension IMAGES\n",
    "# img + src\n",
    "imgs = [img.find('img')['src'].replace(r'.*',r'//upload.*?px-.*?-?_?(.*)') for img in soup.find_all('div', class_='thumbinner')]\n",
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlace = '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to eliminate the link before the image\n",
    "search_links = re.compile('//upload.*?px-.*?-?_?(.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_links.findall(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walt_Disney_envelope_ca._1921.jpg']\n",
      "['seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg']\n",
      "['Trolley_Troubles_poster.jpg']\n",
      "['Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg']\n",
      "['Steamboat-willie.jpg']\n",
      "['Walt_Disney_1935.jpg']\n",
      "['Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg']\n",
      "['Disney_drawing_goofy.jpg']\n",
      "['DisneySchiphol1951.jpg']\n",
      "['WaltDisneyplansDisneylandDec1954.jpg']\n",
      "['Walt_disney_portrait_right.jpg']\n",
      "['Walt_Disney_Grave.JPG']\n",
      "['Roy_O._Disney_with_Company_at_Press_Conference.jpg']\n",
      "['Disney_Display_Case.JPG']\n",
      "['Disney1968.jpg']\n"
     ]
    }
   ],
   "source": [
    "# some text to review. I don't like every term is a list\n",
    "for r in imgs:\n",
    "    print(search_links.findall(r))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in imgs:\n",
    "#     print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension IMAGES\n",
    "# imgs = [img.find('a')['href'].replace(r'/wiki/File:','') for img in soup.find_all('div', class_='thumbinner')]\n",
    "# imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum table image + page images\n",
    "tot_imgs = 1 + len(imgs)\n",
    "tot_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Walt Disney's wikipedia page there are 16 images\n"
     ]
    }
   ],
   "source": [
    "print(f\"In Walt Disney's wikipedia page there are {tot_imgs} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "r = requests.get(url3)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Python - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"6b140b43-59bb-4dbc-abde-d705e0a874ae\",\"wgCSPNonce\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#mw-head\n",
      "#searchInput\n",
      "https://en.wiktionary.org/wiki/Python\n",
      "https://en.wiktionary.org/wiki/python\n",
      "#Snakes\n",
      "#Ancient_Greece\n",
      "#Media_and_entertainment\n",
      "#Computing\n",
      "#Engineering\n",
      "#Roller_coasters\n",
      "#Vehicles\n",
      "#Weaponry\n",
      "#People\n",
      "#Other_uses\n",
      "#See_also\n",
      "/w/index.php?title=Python&action=edit&section=1\n",
      "/wiki/Pythonidae\n",
      "/wiki/Python_(genus)\n",
      "/w/index.php?title=Python&action=edit&section=2\n",
      "/wiki/Python_(mythology)\n",
      "/wiki/Python_of_Aenus\n",
      "/wiki/Python_(painter)\n",
      "/wiki/Python_of_Byzantium\n",
      "/wiki/Python_of_Catana\n",
      "/w/index.php?title=Python&action=edit&section=3\n",
      "/wiki/Python_(film)\n",
      "/wiki/Pythons_2\n",
      "/wiki/Monty_Python\n",
      "/wiki/Python_(Monty)_Pictures\n",
      "/w/index.php?title=Python&action=edit&section=4\n",
      "/wiki/Python_(programming_language)\n",
      "/wiki/CPython\n",
      "/wiki/CMU_Common_Lisp\n",
      "/wiki/PERQ#PERQ_3\n",
      "/w/index.php?title=Python&action=edit&section=5\n",
      "/w/index.php?title=Python&action=edit&section=6\n",
      "/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "/wiki/Python_(Efteling)\n",
      "/w/index.php?title=Python&action=edit&section=7\n",
      "/wiki/Python_(automobile_maker)\n",
      "/wiki/Python_(Ford_prototype)\n",
      "/w/index.php?title=Python&action=edit&section=8\n",
      "/wiki/Colt_Python\n",
      "/wiki/Python_(missile)\n",
      "/wiki/Python_(nuclear_primary)\n",
      "/w/index.php?title=Python&action=edit&section=9\n",
      "/wiki/Python_Anghelo\n",
      "/w/index.php?title=Python&action=edit&section=10\n",
      "/wiki/PYTHON\n",
      "/w/index.php?title=Python&action=edit&section=11\n",
      "/wiki/Cython\n",
      "/wiki/Pyton\n",
      "/wiki/File:Disambig_gray.svg\n",
      "/wiki/Help:Disambiguation\n",
      "https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0\n",
      "https://en.wikipedia.org/w/index.php?title=Python&oldid=963092579\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:Disambiguation_pages\n",
      "/wiki/Category:Disambiguation_pages_with_short_descriptions\n",
      "/wiki/Category:Short_description_is_different_from_Wikidata\n",
      "/wiki/Category:All_article_disambiguation_pages\n",
      "/wiki/Category:All_disambiguation_pages\n",
      "/wiki/Category:Animal_common_name_disambiguation_pages\n",
      "/wiki/Special:MyTalk\n",
      "/wiki/Special:MyContributions\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Python\n",
      "/w/index.php?title=Special:UserLogin&returnto=Python\n",
      "/wiki/Python\n",
      "/wiki/Talk:Python\n",
      "/wiki/Python\n",
      "/w/index.php?title=Python&action=edit\n",
      "/w/index.php?title=Python&action=history\n",
      "/wiki/Main_Page\n",
      "/wiki/Main_Page\n",
      "/wiki/Wikipedia:Contents\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "/wiki/Wikipedia:About\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "//shop.wikimedia.org\n",
      "/wiki/Help:Contents\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:WhatLinksHere/Python\n",
      "/wiki/Special:RecentChangesLinked/Python\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/w/index.php?title=Python&oldid=963092579\n",
      "/w/index.php?title=Python&action=info\n",
      "/w/index.php?title=Special:CiteThisPage&page=Python&id=963092579&wpFormIdentifier=titleform\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452\n",
      "/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen\n",
      "/w/index.php?title=Python&printable=yes\n",
      "https://commons.wikimedia.org/wiki/Category:Python\n",
      "https://af.wikipedia.org/wiki/Python\n",
      "https://als.wikipedia.org/wiki/Python\n",
      "https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86\n",
      "https://az.wikipedia.org/wiki/Python\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)\n",
      "https://be.wikipedia.org/wiki/Python\n",
      "https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)\n",
      "https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)\n",
      "https://da.wikipedia.org/wiki/Python\n",
      "https://de.wikipedia.org/wiki/Python\n",
      "https://eo.wikipedia.org/wiki/Pitono_(apartigilo)\n",
      "https://eu.wikipedia.org/wiki/Python_(argipena)\n",
      "https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86\n",
      "https://fr.wikipedia.org/wiki/Python\n",
      "https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0\n",
      "https://hr.wikipedia.org/wiki/Python_(razdvojba)\n",
      "https://io.wikipedia.org/wiki/Pitono\n",
      "https://id.wikipedia.org/wiki/Python\n",
      "https://ia.wikipedia.org/wiki/Python_(disambiguation)\n",
      "https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)\n",
      "https://it.wikipedia.org/wiki/Python_(disambigua)\n",
      "https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F\n",
      "https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)\n",
      "https://kg.wikipedia.org/wiki/Mboma_(nyoka)\n",
      "https://la.wikipedia.org/wiki/Python_(discretiva)\n",
      "https://lb.wikipedia.org/wiki/Python\n",
      "https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)\n",
      "https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)\n",
      "https://nl.wikipedia.org/wiki/Python\n",
      "https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3\n",
      "https://no.wikipedia.org/wiki/Pyton\n",
      "https://pl.wikipedia.org/wiki/Pyton\n",
      "https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)\n",
      "https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)\n",
      "https://sk.wikipedia.org/wiki/Python\n",
      "https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)\n",
      "https://sh.wikipedia.org/wiki/Python\n",
      "https://fi.wikipedia.org/wiki/Python\n",
      "https://sv.wikipedia.org/wiki/Pyton\n",
      "https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99\n",
      "https://tr.wikipedia.org/wiki/Python\n",
      "https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD\n",
      "https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86\n",
      "https://vi.wikipedia.org/wiki/Python\n",
      "https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
      "//creativecommons.org/licenses/by-sa/3.0/\n",
      "//foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "for link in soup.findAll(\"a\"):\n",
    "    # all the links of the wiki page\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url4 = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "r = requests.get(url4)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version='1.0' encoding='UTF-8' ?>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\"><head>\n",
      "        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
      "        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=8\" />\n",
      "        <meta http-equiv=\"pragma\" content=\"no-cache\" /><!-- HTTP 1.0 -->\n",
      "        <meta http-equiv=\"cache-control\" content=\"no-cache,must-revalidate\" \n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title 5 - Government Organization and Employees ٭'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = soup.find('div', class_='usctitlechanged').text.replace('\\n','').strip()\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# list comprehension TITLE HAVE BEEN CHANGED from the next release point\n",
    "txts = [t.text.replace('\\n','').strip() for t in soup.find_all('div', class_='usctitlechanged')]\n",
    "txts\n",
    "print(len(txts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url5 = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "r = requests.get(url5)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" data-gridsystem=\"bs3\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\">\n",
      "<meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\n",
      "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "<link rel=\"canonical\" href=\"https://www.fbi.gov/wanted/topten\"><title>Ten Most Wanted Fugitives &#8212; FBI</title>\n",
      "<link rel=\"alternate\" href=\"https://www.fbi.gov/wanted/topten/RSS\" title=\"Ten Most Wanted Fugitives - RSS 1.0\" type=\"application/rss+xml\">\n",
      "<link rel=\"alternate\" href=\"https:/\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALEXIS FLORES'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwf = soup.find('h3', class_='title').text.replace('\\n','')\n",
    "mwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'YASER ABDEL SAID',\n",
       " 'SANTIAGO VILLALBA MEDEROS']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehension FUGITIVES\n",
    "mw_fugitives = [mwf.text.replace('\\n','') for mwf in soup.find_all('h3', class_='title')]\n",
    "mw_fugitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mw_fugitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url6 = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "r = requests.get(url6)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:og=\"http://opengraphprotocol.org/schema/\" xml:lang=\"en\" lang=\"en\">\n",
      "<head><meta name=\"google-site-verification\" content=\"srFzNKBTd0FbRhtnzP--Tjxl01NfbscjYwkp4yOWuQY\" /><meta name=\"msvalidate.01\" content=\"BCAA3C04C41AE6E6AFAF117B9469C66F\" /><meta name=\"y_key\" content=\"43b36314ccb77957\" /><!-- 5-Clk8f50tFFdPTU97Bw7ygWE1A -->\n",
      "<meta http-equ\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\")\n",
    "#  tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tables[3]\n",
    "tab_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                        for row in table.find_all(\"tr\")]\n",
    "# tab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CitizenResponse</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Date &amp; Time UTC</td>\n",
       "      <td>Latitude degrees</td>\n",
       "      <td>Longitude degrees</td>\n",
       "      <td>Depth km</td>\n",
       "      <td>Mag  [+]</td>\n",
       "      <td>Region name  [+]</td>\n",
       "      <td>Last update [-]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12345678910›»</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   18:01:46.015min ago</td>\n",
       "      <td>23.12</td>\n",
       "      <td>S</td>\n",
       "      <td>68.96</td>\n",
       "      <td>W</td>\n",
       "      <td>110</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>2020-08-16 18:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   17:30:52.046min ago</td>\n",
       "      <td>24.57</td>\n",
       "      <td>N</td>\n",
       "      <td>94.70</td>\n",
       "      <td>E</td>\n",
       "      <td>35</td>\n",
       "      <td>M</td>\n",
       "      <td>2.8</td>\n",
       "      <td>MYANMAR-INDIA BORDER REGION</td>\n",
       "      <td>2020-08-16 17:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   17:27:57.049min ago</td>\n",
       "      <td>32.92</td>\n",
       "      <td>S</td>\n",
       "      <td>69.07</td>\n",
       "      <td>W</td>\n",
       "      <td>13</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>MENDOZA, ARGENTINA</td>\n",
       "      <td>2020-08-16 17:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   17:00:52.61hr 16min ago</td>\n",
       "      <td>35.96</td>\n",
       "      <td>N</td>\n",
       "      <td>117.03</td>\n",
       "      <td>W</td>\n",
       "      <td>4</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "      <td>2020-08-16 17:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:45:21.01hr 32min ago</td>\n",
       "      <td>33.19</td>\n",
       "      <td>S</td>\n",
       "      <td>70.00</td>\n",
       "      <td>W</td>\n",
       "      <td>11</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.8</td>\n",
       "      <td>MENDOZA, ARGENTINA</td>\n",
       "      <td>2020-08-16 17:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:40:41.91hr 36min ago</td>\n",
       "      <td>30.40</td>\n",
       "      <td>N</td>\n",
       "      <td>94.84</td>\n",
       "      <td>E</td>\n",
       "      <td>20</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.7</td>\n",
       "      <td>EASTERN XIZANG</td>\n",
       "      <td>2020-08-16 17:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:21:49.81hr 55min ago</td>\n",
       "      <td>20.77</td>\n",
       "      <td>S</td>\n",
       "      <td>173.78</td>\n",
       "      <td>W</td>\n",
       "      <td>10</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.9</td>\n",
       "      <td>TONGA</td>\n",
       "      <td>2020-08-16 17:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:20:36.01hr 56min ago</td>\n",
       "      <td>18.90</td>\n",
       "      <td>N</td>\n",
       "      <td>121.47</td>\n",
       "      <td>E</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>3.4</td>\n",
       "      <td>LUZON, PHILIPPINES</td>\n",
       "      <td>2020-08-16 16:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:17:19.02hr 00min ago</td>\n",
       "      <td>33.27</td>\n",
       "      <td>S</td>\n",
       "      <td>71.76</td>\n",
       "      <td>W</td>\n",
       "      <td>30</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.5</td>\n",
       "      <td>OFFSHORE VALPARAISO, CHILE</td>\n",
       "      <td>2020-08-16 16:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:11:02.52hr 06min ago</td>\n",
       "      <td>0.28</td>\n",
       "      <td>S</td>\n",
       "      <td>125.24</td>\n",
       "      <td>E</td>\n",
       "      <td>40</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.7</td>\n",
       "      <td>MOLUCCA SEA</td>\n",
       "      <td>2020-08-16 17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:09:54.12hr 07min ago</td>\n",
       "      <td>19.21</td>\n",
       "      <td>N</td>\n",
       "      <td>155.40</td>\n",
       "      <td>W</td>\n",
       "      <td>32</td>\n",
       "      <td>MD</td>\n",
       "      <td>2.1</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2020-08-16 16:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:08:02.02hr 09min ago</td>\n",
       "      <td>23.88</td>\n",
       "      <td>S</td>\n",
       "      <td>67.47</td>\n",
       "      <td>W</td>\n",
       "      <td>245</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>2020-08-16 17:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:06:32.52hr 10min ago</td>\n",
       "      <td>45.59</td>\n",
       "      <td>N</td>\n",
       "      <td>26.55</td>\n",
       "      <td>E</td>\n",
       "      <td>103</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.8</td>\n",
       "      <td>ROMANIA</td>\n",
       "      <td>2020-08-16 17:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:01:30.32hr 16min ago</td>\n",
       "      <td>39.12</td>\n",
       "      <td>N</td>\n",
       "      <td>27.77</td>\n",
       "      <td>E</td>\n",
       "      <td>7</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.6</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "      <td>2020-08-16 16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   15:47:23.62hr 30min ago</td>\n",
       "      <td>19.40</td>\n",
       "      <td>N</td>\n",
       "      <td>155.27</td>\n",
       "      <td>W</td>\n",
       "      <td>17</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.1</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2020-08-16 16:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0     1     2  \\\n",
       "0                    None  None   \n",
       "1   CitizenResponse               \n",
       "2                                 \n",
       "3                    None  None   \n",
       "4     12345678910›»  None  None   \n",
       "5                                 \n",
       "6                                 \n",
       "7                                 \n",
       "8                                 \n",
       "9                                 \n",
       "10                                \n",
       "11                                \n",
       "12                                \n",
       "13                                \n",
       "14                                \n",
       "15                                \n",
       "16                                \n",
       "17                                \n",
       "18                                \n",
       "19                                \n",
       "\n",
       "                                                 3                4  \\\n",
       "0                                             None             None   \n",
       "1                                                   Date & Time UTC   \n",
       "2                                             None             None   \n",
       "3                                             None             None   \n",
       "4                                             None             None   \n",
       "5       earthquake2020-08-16   18:01:46.015min ago           23.12    \n",
       "6       earthquake2020-08-16   17:30:52.046min ago           24.57    \n",
       "7       earthquake2020-08-16   17:27:57.049min ago           32.92    \n",
       "8   earthquake2020-08-16   17:00:52.61hr 16min ago           35.96    \n",
       "9   earthquake2020-08-16   16:45:21.01hr 32min ago           33.19    \n",
       "10  earthquake2020-08-16   16:40:41.91hr 36min ago           30.40    \n",
       "11  earthquake2020-08-16   16:21:49.81hr 55min ago           20.77    \n",
       "12  earthquake2020-08-16   16:20:36.01hr 56min ago           18.90    \n",
       "13  earthquake2020-08-16   16:17:19.02hr 00min ago           33.27    \n",
       "14  earthquake2020-08-16   16:11:02.52hr 06min ago            0.28    \n",
       "15  earthquake2020-08-16   16:09:54.12hr 07min ago           19.21    \n",
       "16  earthquake2020-08-16   16:08:02.02hr 09min ago           23.88    \n",
       "17  earthquake2020-08-16   16:06:32.52hr 10min ago           45.59    \n",
       "18  earthquake2020-08-16   16:01:30.32hr 16min ago           39.12    \n",
       "19  earthquake2020-08-16   15:47:23.62hr 30min ago           19.40    \n",
       "\n",
       "                   5                  6         7         8                 9  \\\n",
       "0               None               None      None      None              None   \n",
       "1   Latitude degrees  Longitude degrees  Depth km  Mag  [+]  Region name  [+]   \n",
       "2               None               None      None      None              None   \n",
       "3               None               None      None      None              None   \n",
       "4               None               None      None      None              None   \n",
       "5                S               68.96        W         110                ML   \n",
       "6                N               94.70        E          35                 M   \n",
       "7                S               69.07        W          13                ML   \n",
       "8                N              117.03        W           4                ML   \n",
       "9                S               70.00        W          11                ML   \n",
       "10               N               94.84        E          20                mb   \n",
       "11               S              173.78        W          10                mb   \n",
       "12               N              121.47        E          26                 M   \n",
       "13               S               71.76        W          30                ML   \n",
       "14               S              125.24        E          40                mb   \n",
       "15               N              155.40        W          32                MD   \n",
       "16               S               67.47        W         245                ML   \n",
       "17               N               26.55        E         103                ML   \n",
       "18               N               27.77        E           7                ML   \n",
       "19               N              155.27        W          17                ML   \n",
       "\n",
       "                 10                            11                12  \n",
       "0              None                          None              None  \n",
       "1   Last update [-]                          None              None  \n",
       "2              None                          None              None  \n",
       "3              None                          None              None  \n",
       "4              None                          None              None  \n",
       "5               2.7            ANTOFAGASTA, CHILE  2020-08-16 18:16  \n",
       "6               2.8   MYANMAR-INDIA BORDER REGION  2020-08-16 17:41  \n",
       "7               2.7            MENDOZA, ARGENTINA  2020-08-16 17:43  \n",
       "8               2.6            CENTRAL CALIFORNIA  2020-08-16 17:26  \n",
       "9               2.8            MENDOZA, ARGENTINA  2020-08-16 17:20  \n",
       "10              4.7                EASTERN XIZANG  2020-08-16 17:25  \n",
       "11              4.9                         TONGA  2020-08-16 17:03  \n",
       "12              3.4            LUZON, PHILIPPINES  2020-08-16 16:30  \n",
       "13              2.5    OFFSHORE VALPARAISO, CHILE  2020-08-16 16:43  \n",
       "14              4.7                   MOLUCCA SEA  2020-08-16 17:00  \n",
       "15              2.1      ISLAND OF HAWAII, HAWAII  2020-08-16 16:13  \n",
       "16              2.7            ANTOFAGASTA, CHILE  2020-08-16 17:29  \n",
       "17              2.8                       ROMANIA  2020-08-16 17:08  \n",
       "18              2.6                WESTERN TURKEY  2020-08-16 16:09  \n",
       "19              3.1      ISLAND OF HAWAII, HAWAII  2020-08-16 16:39  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tab_data)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the first row to the headers\n",
    "df.columns = df.iloc[1,:]\n",
    "df.drop(index=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete lines with null columns\n",
    "df = df.dropna(how='any', thresh = 10,axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>1</th>\n",
       "      <th>CitizenResponse</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time UTC</th>\n",
       "      <th>Latitude degrees</th>\n",
       "      <th>Longitude degrees</th>\n",
       "      <th>Depth km</th>\n",
       "      <th>Mag  [+]</th>\n",
       "      <th>Region name  [+]</th>\n",
       "      <th>Last update [-]</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   18:01:46.015min ago</td>\n",
       "      <td>23.12</td>\n",
       "      <td>S</td>\n",
       "      <td>68.96</td>\n",
       "      <td>W</td>\n",
       "      <td>110</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>2020-08-16 18:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   17:30:52.046min ago</td>\n",
       "      <td>24.57</td>\n",
       "      <td>N</td>\n",
       "      <td>94.70</td>\n",
       "      <td>E</td>\n",
       "      <td>35</td>\n",
       "      <td>M</td>\n",
       "      <td>2.8</td>\n",
       "      <td>MYANMAR-INDIA BORDER REGION</td>\n",
       "      <td>2020-08-16 17:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   17:27:57.049min ago</td>\n",
       "      <td>32.92</td>\n",
       "      <td>S</td>\n",
       "      <td>69.07</td>\n",
       "      <td>W</td>\n",
       "      <td>13</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>MENDOZA, ARGENTINA</td>\n",
       "      <td>2020-08-16 17:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   17:00:52.61hr 16min ago</td>\n",
       "      <td>35.96</td>\n",
       "      <td>N</td>\n",
       "      <td>117.03</td>\n",
       "      <td>W</td>\n",
       "      <td>4</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "      <td>2020-08-16 17:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:45:21.01hr 32min ago</td>\n",
       "      <td>33.19</td>\n",
       "      <td>S</td>\n",
       "      <td>70.00</td>\n",
       "      <td>W</td>\n",
       "      <td>11</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.8</td>\n",
       "      <td>MENDOZA, ARGENTINA</td>\n",
       "      <td>2020-08-16 17:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:40:41.91hr 36min ago</td>\n",
       "      <td>30.40</td>\n",
       "      <td>N</td>\n",
       "      <td>94.84</td>\n",
       "      <td>E</td>\n",
       "      <td>20</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.7</td>\n",
       "      <td>EASTERN XIZANG</td>\n",
       "      <td>2020-08-16 17:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:21:49.81hr 55min ago</td>\n",
       "      <td>20.77</td>\n",
       "      <td>S</td>\n",
       "      <td>173.78</td>\n",
       "      <td>W</td>\n",
       "      <td>10</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.9</td>\n",
       "      <td>TONGA</td>\n",
       "      <td>2020-08-16 17:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:20:36.01hr 56min ago</td>\n",
       "      <td>18.90</td>\n",
       "      <td>N</td>\n",
       "      <td>121.47</td>\n",
       "      <td>E</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>3.4</td>\n",
       "      <td>LUZON, PHILIPPINES</td>\n",
       "      <td>2020-08-16 16:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:17:19.02hr 00min ago</td>\n",
       "      <td>33.27</td>\n",
       "      <td>S</td>\n",
       "      <td>71.76</td>\n",
       "      <td>W</td>\n",
       "      <td>30</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.5</td>\n",
       "      <td>OFFSHORE VALPARAISO, CHILE</td>\n",
       "      <td>2020-08-16 16:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:11:02.52hr 06min ago</td>\n",
       "      <td>0.28</td>\n",
       "      <td>S</td>\n",
       "      <td>125.24</td>\n",
       "      <td>E</td>\n",
       "      <td>40</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.7</td>\n",
       "      <td>MOLUCCA SEA</td>\n",
       "      <td>2020-08-16 17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:09:54.12hr 07min ago</td>\n",
       "      <td>19.21</td>\n",
       "      <td>N</td>\n",
       "      <td>155.40</td>\n",
       "      <td>W</td>\n",
       "      <td>32</td>\n",
       "      <td>MD</td>\n",
       "      <td>2.1</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2020-08-16 16:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:08:02.02hr 09min ago</td>\n",
       "      <td>23.88</td>\n",
       "      <td>S</td>\n",
       "      <td>67.47</td>\n",
       "      <td>W</td>\n",
       "      <td>245</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>2020-08-16 17:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:06:32.52hr 10min ago</td>\n",
       "      <td>45.59</td>\n",
       "      <td>N</td>\n",
       "      <td>26.55</td>\n",
       "      <td>E</td>\n",
       "      <td>103</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.8</td>\n",
       "      <td>ROMANIA</td>\n",
       "      <td>2020-08-16 17:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   16:01:30.32hr 16min ago</td>\n",
       "      <td>39.12</td>\n",
       "      <td>N</td>\n",
       "      <td>27.77</td>\n",
       "      <td>E</td>\n",
       "      <td>7</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.6</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "      <td>2020-08-16 16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   15:47:23.62hr 30min ago</td>\n",
       "      <td>19.40</td>\n",
       "      <td>N</td>\n",
       "      <td>155.27</td>\n",
       "      <td>W</td>\n",
       "      <td>17</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.1</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2020-08-16 16:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   15:46:05.02hr 31min ago</td>\n",
       "      <td>38.54</td>\n",
       "      <td>N</td>\n",
       "      <td>122.64</td>\n",
       "      <td>W</td>\n",
       "      <td>3</td>\n",
       "      <td>Md</td>\n",
       "      <td>2.1</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "      <td>2020-08-16 15:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>earthquake2020-08-16   15:45:41.02hr 31min ago</td>\n",
       "      <td>19.41</td>\n",
       "      <td>N</td>\n",
       "      <td>155.26</td>\n",
       "      <td>W</td>\n",
       "      <td>17</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.9</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2020-08-16 15:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   15:44:56.32hr 32min ago</td>\n",
       "      <td>31.97</td>\n",
       "      <td>S</td>\n",
       "      <td>117.20</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.5</td>\n",
       "      <td>WESTERN AUSTRALIA</td>\n",
       "      <td>2020-08-16 15:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   15:33:20.02hr 44min ago</td>\n",
       "      <td>21.80</td>\n",
       "      <td>S</td>\n",
       "      <td>68.48</td>\n",
       "      <td>W</td>\n",
       "      <td>122</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>2020-08-16 15:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake2020-08-16   15:28:01.22hr 49min ago</td>\n",
       "      <td>38.79</td>\n",
       "      <td>N</td>\n",
       "      <td>122.79</td>\n",
       "      <td>W</td>\n",
       "      <td>0</td>\n",
       "      <td>Md</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "      <td>2020-08-16 15:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "1  CitizenResponse                                                       \\\n",
       "0                            earthquake2020-08-16   18:01:46.015min ago   \n",
       "1                            earthquake2020-08-16   17:30:52.046min ago   \n",
       "2                            earthquake2020-08-16   17:27:57.049min ago   \n",
       "3                        earthquake2020-08-16   17:00:52.61hr 16min ago   \n",
       "4                        earthquake2020-08-16   16:45:21.01hr 32min ago   \n",
       "5                        earthquake2020-08-16   16:40:41.91hr 36min ago   \n",
       "6                        earthquake2020-08-16   16:21:49.81hr 55min ago   \n",
       "7                        earthquake2020-08-16   16:20:36.01hr 56min ago   \n",
       "8                        earthquake2020-08-16   16:17:19.02hr 00min ago   \n",
       "9                        earthquake2020-08-16   16:11:02.52hr 06min ago   \n",
       "10                       earthquake2020-08-16   16:09:54.12hr 07min ago   \n",
       "11                       earthquake2020-08-16   16:08:02.02hr 09min ago   \n",
       "12                       earthquake2020-08-16   16:06:32.52hr 10min ago   \n",
       "13                       earthquake2020-08-16   16:01:30.32hr 16min ago   \n",
       "14                       earthquake2020-08-16   15:47:23.62hr 30min ago   \n",
       "15                       earthquake2020-08-16   15:46:05.02hr 31min ago   \n",
       "16               1    F  earthquake2020-08-16   15:45:41.02hr 31min ago   \n",
       "17                       earthquake2020-08-16   15:44:56.32hr 32min ago   \n",
       "18                       earthquake2020-08-16   15:33:20.02hr 44min ago   \n",
       "19                       earthquake2020-08-16   15:28:01.22hr 49min ago   \n",
       "\n",
       "1  Date & Time UTC Latitude degrees Longitude degrees Depth km Mag  [+]  \\\n",
       "0           23.12               S              68.96       W        110   \n",
       "1           24.57               N              94.70       E         35   \n",
       "2           32.92               S              69.07       W         13   \n",
       "3           35.96               N             117.03       W          4   \n",
       "4           33.19               S              70.00       W         11   \n",
       "5           30.40               N              94.84       E         20   \n",
       "6           20.77               S             173.78       W         10   \n",
       "7           18.90               N             121.47       E         26   \n",
       "8           33.27               S              71.76       W         30   \n",
       "9            0.28               S             125.24       E         40   \n",
       "10          19.21               N             155.40       W         32   \n",
       "11          23.88               S              67.47       W        245   \n",
       "12          45.59               N              26.55       E        103   \n",
       "13          39.12               N              27.77       E          7   \n",
       "14          19.40               N             155.27       W         17   \n",
       "15          38.54               N             122.64       W          3   \n",
       "16          19.41               N             155.26       W         17   \n",
       "17          31.97               S             117.20       E          5   \n",
       "18          21.80               S              68.48       W        122   \n",
       "19          38.79               N             122.79       W          0   \n",
       "\n",
       "1  Region name  [+] Last update [-]                           NaN  \\\n",
       "0                ML             2.7            ANTOFAGASTA, CHILE   \n",
       "1                 M             2.8   MYANMAR-INDIA BORDER REGION   \n",
       "2                ML             2.7            MENDOZA, ARGENTINA   \n",
       "3                ML             2.6            CENTRAL CALIFORNIA   \n",
       "4                ML             2.8            MENDOZA, ARGENTINA   \n",
       "5                mb             4.7                EASTERN XIZANG   \n",
       "6                mb             4.9                         TONGA   \n",
       "7                 M             3.4            LUZON, PHILIPPINES   \n",
       "8                ML             2.5    OFFSHORE VALPARAISO, CHILE   \n",
       "9                mb             4.7                   MOLUCCA SEA   \n",
       "10               MD             2.1      ISLAND OF HAWAII, HAWAII   \n",
       "11               ML             2.7            ANTOFAGASTA, CHILE   \n",
       "12               ML             2.8                       ROMANIA   \n",
       "13               ML             2.6                WESTERN TURKEY   \n",
       "14               ML             3.1      ISLAND OF HAWAII, HAWAII   \n",
       "15               Md             2.1           NORTHERN CALIFORNIA   \n",
       "16               ML             3.9      ISLAND OF HAWAII, HAWAII   \n",
       "17               ML             2.5             WESTERN AUSTRALIA   \n",
       "18               ML             3.0            ANTOFAGASTA, CHILE   \n",
       "19               Md             2.0           NORTHERN CALIFORNIA   \n",
       "\n",
       "1                NaN  \n",
       "0   2020-08-16 18:16  \n",
       "1   2020-08-16 17:41  \n",
       "2   2020-08-16 17:43  \n",
       "3   2020-08-16 17:26  \n",
       "4   2020-08-16 17:20  \n",
       "5   2020-08-16 17:25  \n",
       "6   2020-08-16 17:03  \n",
       "7   2020-08-16 16:30  \n",
       "8   2020-08-16 16:43  \n",
       "9   2020-08-16 17:00  \n",
       "10  2020-08-16 16:13  \n",
       "11  2020-08-16 17:29  \n",
       "12  2020-08-16 17:08  \n",
       "13  2020-08-16 16:09  \n",
       "14  2020-08-16 16:39  \n",
       "15  2020-08-16 15:47  \n",
       "16  2020-08-16 15:58  \n",
       "17  2020-08-16 15:51  \n",
       "18  2020-08-16 15:44  \n",
       "19  2020-08-16 15:29  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "del df['index']\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url7 = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your account name on Twitter:  @NBA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account name not found...\n"
     ]
    }
   ],
   "source": [
    "# w3resource.com\n",
    "# https://www.w3resource.com/python-exercises/web-scraping/web-scraping-exercise-19.php\n",
    "\n",
    "handle = input('Input your account name on Twitter: ')\n",
    "temp = requests.get(url7+handle)\n",
    "soup = bs(temp.text,'lxml')\n",
    "\n",
    "try:\n",
    "    tweet_box = soup.find('li',{'class':'ProfileNav-item ProfileNav-item--tweets is-active'})\n",
    "    tweets= tweet_box.find('a').find('span',{'class':'ProfileNav-value'})\n",
    "    print(\"{} tweets {} number of tweets.\".format(handle,tweets.get('data-count')))\n",
    "\n",
    "except:\n",
    "    print('Account name not found...')\n",
    "    \n",
    "# DOESN'T WORK!\n",
    "# I'm not able to find the number of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url8 = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "# r = requests.get(url8)\n",
    "# r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "r = requests.get(url9)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"mul\" class=\"no-js\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\">\n",
      "<title>Wikipedia</title>\n",
      "<meta name=\"description\" content=\"Wikipedia is a free online encyclopedia, created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\">\n",
      "<script>\n",
      "document.documentElement.className = document.documentElement.className.replace( /(^|\\s)no-js(\\s|$)/, \"$1js-enabled$2\" );\n",
      "</script>\n",
      "<meta name=\"viewport\" content=\"initial-scale=1,user-scalable=yes\">\n",
      "<link rel=\"apple-touch\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = soup.find('a', class_ = 'link-box').find('strong').text\n",
    "lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.137.000+ articles'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art = soup.find('a', class_ = 'link-box').find('small').text.replace('\\xa0','.')\n",
    "art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English', '6.137.000+ articles'),\n",
       " ('æ\\x97¥æ\\x9c¬èª\\x9e', '1.222.000+ è¨\\x98äº\\x8b'),\n",
       " ('EspaÃ±ol', '1.617.000+ artÃ\\xadculos'),\n",
       " ('Deutsch', '2.467.000+ Artikel'),\n",
       " ('Ð\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹', '1.651.000+ Ñ\\x81Ñ\\x82Ð°Ñ\\x82ÐµÐ¹'),\n",
       " ('FranÃ§ais', '2.241.000+ articles'),\n",
       " ('Italiano', '1.627.000+ voci'),\n",
       " ('ä¸\\xadæ\\x96\\x87', '1.136.000+ æ¢\\x9dç\\x9b®'),\n",
       " ('PortuguÃªs', '1.041.000+ artigos'),\n",
       " ('Polski', '1.423.000+ haseÅ\\x82')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia = [(wiki.find('strong').text, wiki.find('small').text.replace('\\xa0','.')) for wiki in soup.find_all('a', class_ = 'link-box')]\n",
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English\n",
      "6 137 000+ articles\n",
      "\n",
      "\n",
      "æ¥æ¬èª\n",
      "1 222 000+ è¨äº\n",
      "\n",
      "\n",
      "EspaÃ±ol\n",
      "1 617 000+ artÃ­culos\n",
      "\n",
      "\n",
      "Deutsch\n",
      "2 467 000+ Artikel\n",
      "\n",
      "\n",
      "Ð ÑÑÑÐºÐ¸Ð¹\n",
      "1 651 000+ ÑÑÐ°ÑÐµÐ¹\n",
      "\n",
      "\n",
      "FranÃ§ais\n",
      "2 241 000+ articles\n",
      "\n",
      "\n",
      "Italiano\n",
      "1 627 000+ voci\n",
      "\n",
      "\n",
      "ä¸­æ\n",
      "1 136 000+ æ¢ç®\n",
      "\n",
      "\n",
      "PortuguÃªs\n",
      "1 041 000+ artigos\n",
      "\n",
      "\n",
      "Polski\n",
      "1 423 000+ haseÅ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ... an easy way to do it!\n",
    "for wiki in soup.find_all('a', class_ = 'link-box'):\n",
    "    print(wiki.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url10 = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "r = requests.get(url10)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 9]><html class=\"lte-ie8\" lang=\"en\"><![endif]-->\n",
      "<!--[if gt IE 8]><!--><html lang=\"en\"><!--<![endif]-->\n",
      "<html class=\"govuk-template\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\">\n",
      "    <title>Find open data - data.gov.uk</title>\n",
      "\n",
      "    <meta name=\"theme-color\" content=\"#0b0c0c\" />\n",
      "\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "    \n",
      "    <link rel=\"stylesheet\" media=\"screen\" href=\"/find-assets/application-05aa6420d403adc1fdc7a0dc1fb860dc097af29300239c7e5\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/cookies', '/cookies', 'http://www.smartsurvey.co.uk/s/3SEXD/', '/search?filters%5Btopic%5D=Business+and+economy', '/search?filters%5Btopic%5D=Crime+and+justice', '/search?filters%5Btopic%5D=Defence', '/search?filters%5Btopic%5D=Education', '/search?filters%5Btopic%5D=Environment', '/search?filters%5Btopic%5D=Government', '/search?filters%5Btopic%5D=Government+spending', '/search?filters%5Btopic%5D=Health', '/search?filters%5Btopic%5D=Mapping', '/search?filters%5Btopic%5D=Society', '/search?filters%5Btopic%5D=Towns+and+cities', '/search?filters%5Btopic%5D=Transport']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in soup.findAll(\"a\", class_ = 'govuk-link'):\n",
    "    if 'href' in i.attrs:\n",
    "        data.append(i.attrs['href'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business+and+economy\n",
      "Crime+and+justice\n",
      "Defence\n",
      "Education\n",
      "Environment\n",
      "Government\n",
      "Government+spending\n",
      "Health\n",
      "Mapping\n",
      "Society\n",
      "Towns+and+cities\n",
      "Transport\n"
     ]
    }
   ],
   "source": [
    "# eliminate the first three elements of the list and modify strings\n",
    "for d in data[3:]:\n",
    "    dataset = re.sub(r'.*?=', '', d)  \n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business and economy\n",
      "Crime and justice\n",
      "Defence\n",
      "Education\n",
      "Environment\n",
      "Government\n",
      "Government spending\n",
      "Health\n",
      "Mapping\n",
      "Society\n",
      "Towns and cities\n",
      "Transport\n"
     ]
    }
   ],
   "source": [
    "# ... an easy way to do it!\n",
    "for tit in soup.findAll(\"h3\"):\n",
    "    print(tit.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url11 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "r = requests.get(url11)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>List of languages by number of native speakers - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"66b378ee-\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the tables in the page\n",
    "tables = soup.find_all(\"table\")\n",
    "# tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the table I need to analyze\n",
    "table = tables[0]\n",
    "tab_data = [[cell.text.replace('\\n','') for cell in row.find_all([\"th\",\"td\"])]\n",
    "                        for row in table.find_all(\"tr\")]\n",
    "# tab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Rank</td>\n",
       "      <td>Language</td>\n",
       "      <td>Speakers(millions)</td>\n",
       "      <td>% of World pop.(March 2019)[8]</td>\n",
       "      <td>Language family</td>\n",
       "      <td>Branch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.994</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (Sanskritised Hindustani)[9]</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>228</td>\n",
       "      <td>2.961</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "      <td>2.870</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>2.000</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "      <td>1.662</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi[10]</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>Telugu</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.065</td>\n",
       "      <td>Dravidian</td>\n",
       "      <td>South-Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Wu Chinese</td>\n",
       "      <td>81.4</td>\n",
       "      <td>1.057</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>79.4</td>\n",
       "      <td>1.031</td>\n",
       "      <td>Turkic</td>\n",
       "      <td>Oghuz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>Korean</td>\n",
       "      <td>77.3</td>\n",
       "      <td>1.004</td>\n",
       "      <td>Koreanic</td>\n",
       "      <td>language isolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>French</td>\n",
       "      <td>77.2</td>\n",
       "      <td>1.003</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>German</td>\n",
       "      <td>76.1</td>\n",
       "      <td>0.988</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.987</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>Tamil</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.974</td>\n",
       "      <td>Dravidian</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>Yue Chinese</td>\n",
       "      <td>73.1</td>\n",
       "      <td>0.949</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                   1                   2  \\\n",
       "0   Rank                            Language  Speakers(millions)   \n",
       "1      1                    Mandarin Chinese                 918   \n",
       "2      2                             Spanish                 480   \n",
       "3      3                             English                 379   \n",
       "4      4  Hindi (Sanskritised Hindustani)[9]                 341   \n",
       "5      5                             Bengali                 228   \n",
       "6      6                          Portuguese                 221   \n",
       "7      7                             Russian                 154   \n",
       "8      8                            Japanese                 128   \n",
       "9      9                 Western Punjabi[10]                92.7   \n",
       "10    10                             Marathi                83.1   \n",
       "11    11                              Telugu                82.0   \n",
       "12    12                          Wu Chinese                81.4   \n",
       "13    13                             Turkish                79.4   \n",
       "14    14                              Korean                77.3   \n",
       "15    15                              French                77.2   \n",
       "16    16                              German                76.1   \n",
       "17    17                          Vietnamese                76.0   \n",
       "18    18                               Tamil                75.0   \n",
       "19    19                         Yue Chinese                73.1   \n",
       "\n",
       "                                 3                4                 5  \n",
       "0   % of World pop.(March 2019)[8]  Language family            Branch  \n",
       "1                           11.922     Sino-Tibetan           Sinitic  \n",
       "2                            5.994    Indo-European           Romance  \n",
       "3                            4.922    Indo-European          Germanic  \n",
       "4                            4.429    Indo-European        Indo-Aryan  \n",
       "5                            2.961    Indo-European        Indo-Aryan  \n",
       "6                            2.870    Indo-European           Romance  \n",
       "7                            2.000    Indo-European      Balto-Slavic  \n",
       "8                            1.662          Japonic          Japanese  \n",
       "9                            1.204    Indo-European        Indo-Aryan  \n",
       "10                           1.079    Indo-European        Indo-Aryan  \n",
       "11                           1.065        Dravidian     South-Central  \n",
       "12                           1.057     Sino-Tibetan           Sinitic  \n",
       "13                           1.031           Turkic             Oghuz  \n",
       "14                           1.004         Koreanic  language isolate  \n",
       "15                           1.003    Indo-European           Romance  \n",
       "16                           0.988    Indo-European          Germanic  \n",
       "17                           0.987    Austroasiatic            Vietic  \n",
       "18                           0.974        Dravidian             South  \n",
       "19                           0.949     Sino-Tibetan           Sinitic  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tab_data)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>% of World pop.(March 2019)[8]</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.994</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (Sanskritised Hindustani)[9]</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>228</td>\n",
       "      <td>2.961</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "      <td>2.870</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>2.000</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "      <td>1.662</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi[10]</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0  Rank                            Language Speakers(millions)  \\\n",
       "1     1                    Mandarin Chinese                918   \n",
       "2     2                             Spanish                480   \n",
       "3     3                             English                379   \n",
       "4     4  Hindi (Sanskritised Hindustani)[9]                341   \n",
       "5     5                             Bengali                228   \n",
       "6     6                          Portuguese                221   \n",
       "7     7                             Russian                154   \n",
       "8     8                            Japanese                128   \n",
       "9     9                 Western Punjabi[10]               92.7   \n",
       "10   10                             Marathi               83.1   \n",
       "\n",
       "0  % of World pop.(March 2019)[8] Language family        Branch  \n",
       "1                          11.922    Sino-Tibetan       Sinitic  \n",
       "2                           5.994   Indo-European       Romance  \n",
       "3                           4.922   Indo-European      Germanic  \n",
       "4                           4.429   Indo-European    Indo-Aryan  \n",
       "5                           2.961   Indo-European    Indo-Aryan  \n",
       "6                           2.870   Indo-European       Romance  \n",
       "7                           2.000   Indo-European  Balto-Slavic  \n",
       "8                           1.662         Japonic      Japanese  \n",
       "9                           1.204   Indo-European    Indo-Aryan  \n",
       "10                          1.079   Indo-European    Indo-Aryan  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.iloc[0,:]\n",
    "df.drop(index=0,inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url12 = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "r = requests.get(url12)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html dir=\"ltr\" lang=\"en\">\n",
      "<meta charset=\"utf-8\" />\n",
      "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover\" />\n",
      "<link rel=\"preconnect\" href=\"//abs.twimg.com\" />\n",
      "<link rel=\"preconnect\" href=\"//api.twitter.com\" />\n",
      "<link rel=\"preconnect\" href=\"//pbs.twimg.com\" />\n",
      "<link rel=\"preconnect\" href=\"//t.co\" />\n",
      "<link rel=\"preconnect\" href=\"//video.twimg.com\" />\n",
      "<link rel=\"dns-prefetch\" href=\"//abs.twimg.com\" />\n",
      "<link rel=\"dns-prefe\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url13 = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "r = requests.get(url13)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html\n",
      "    xmlns:og=\"http://ogp.me/ns#\"\n",
      "    xmlns:fb=\"http://www.facebook.com/2008/fbml\">\n",
      "    <head>\n",
      "         \n",
      "        <meta charset=\"utf-8\">\n",
      "        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "    <meta name=\"apple-itunes-app\" content=\"app-id=342792525, app-argument=imdb:///?src=mdot\">\n",
      "            <style>\n",
      "                body#styleguide-v2 {\n",
      "                    background: no-repeat fixed center top #000;\n",
      "                }\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the tables in the page\n",
    "tables = soup.find_all(\"table\")\n",
    "# tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cadena perpetua'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the movie\n",
    "name = soup.find('td', class_ = 'titleColumn').find('a').text\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frank Darabont (dir.), Tim Robbins, Morgan Freeman'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# director and stars\n",
    "stars = soup.find('td', class_ = 'titleColumn').find('a')['title']\n",
    "stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1994'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# release date\n",
    "date = soup.find('td', class_ = 'titleColumn').find('span', class_ = 'secondaryInfo').text.replace('(','').replace(')','')\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the table I need to analyze\n",
    "table = tables[0]\n",
    "tab_data = [[(cell.find('a').text, cell.find('a')['title'],cell.find('span', class_ = 'secondaryInfo').text.replace('(','').replace(')','')) \n",
    "              for cell in row.find_all(\"td\", class_ = 'titleColumn')]\n",
    "                        for row in table.find_all(\"tr\")]\n",
    "# tab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify cell type\n",
    "# for cell in tab_data:\n",
    "#     print(type(cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tab_data)\n",
    "# splitting a list in a Pandas cell into multiple columns\n",
    "df = df[0].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the first row to the headers\n",
    "df.columns = df.iloc[0,:]\n",
    "df.drop(index=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Director and cast</th>\n",
       "      <th>Date release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>Frank Darabont (dir.), Tim Robbins, Morgan Fre...</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>El padrino</td>\n",
       "      <td>Francis Ford Coppola (dir.), Marlon Brando, Al...</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>Francis Ford Coppola (dir.), Al Pacino, Robert...</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>Christopher Nolan (dir.), Christian Bale, Heat...</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>La batalla de Argel</td>\n",
       "      <td>Gillo Pontecorvo (dir.), Brahim Hadjadj, Jean ...</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>Ashutosh Gowariker (dir.), Shah Rukh Khan, Gay...</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>Trono de sangre</td>\n",
       "      <td>Akira Kurosawa (dir.), Toshirô Mifune, Minoru ...</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>Lagaan: Érase una vez en la India</td>\n",
       "      <td>Ashutosh Gowariker (dir.), Aamir Khan, Raghuvi...</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>Neon Genesis Evangelion: The End of Evangelion</td>\n",
       "      <td>Hideaki Anno (dir.), Megumi Ogata, Megumi Haya...</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Name  \\\n",
       "1                                   Cadena perpetua   \n",
       "2                                        El padrino   \n",
       "3                              El padrino: Parte II   \n",
       "4                               El caballero oscuro   \n",
       "5                             12 hombres sin piedad   \n",
       "..                                              ...   \n",
       "246                             La batalla de Argel   \n",
       "247                          Swades: We, the People   \n",
       "248                                 Trono de sangre   \n",
       "249               Lagaan: Érase una vez en la India   \n",
       "250  Neon Genesis Evangelion: The End of Evangelion   \n",
       "\n",
       "                                     Director and cast Date release  \n",
       "1    Frank Darabont (dir.), Tim Robbins, Morgan Fre...         1994  \n",
       "2    Francis Ford Coppola (dir.), Marlon Brando, Al...         1972  \n",
       "3    Francis Ford Coppola (dir.), Al Pacino, Robert...         1974  \n",
       "4    Christopher Nolan (dir.), Christian Bale, Heat...         2008  \n",
       "5        Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb         1957  \n",
       "..                                                 ...          ...  \n",
       "246  Gillo Pontecorvo (dir.), Brahim Hadjadj, Jean ...         1966  \n",
       "247  Ashutosh Gowariker (dir.), Shah Rukh Khan, Gay...         2004  \n",
       "248  Akira Kurosawa (dir.), Toshirô Mifune, Minoru ...         1957  \n",
       "249  Ashutosh Gowariker (dir.), Aamir Khan, Raghuvi...         2001  \n",
       "250  Hideaki Anno (dir.), Megumi Ogata, Megumi Haya...         1997  \n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "df.columns = ['Name', 'Director and cast', 'Date release']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url14 = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "r = requests.get(url14)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html\n",
      "    xmlns:og=\"http://ogp.me/ns#\"\n",
      "    xmlns:fb=\"http://www.facebook.com/2008/fbml\">\n",
      "    <head>\n",
      "         \n",
      "        <meta charset=\"utf-8\">\n",
      "        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "    <meta name=\"apple-itunes-app\" content=\"app-id=342792525, app-argument=imdb:///?src=mdot\">\n",
      "            <style>\n",
      "                body#styleguide-v2 {\n",
      "                    background: no-repeat fixed center top #000;\n",
      "                }\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not able to find the summary of the movies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table with name and year of the top 10 random movies\n",
    "table1 = tables[0]\n",
    "tab_data1 = [[(cell.find('a').text, cell.find('span', class_ = 'secondaryInfo').text.replace('(','').replace(')',''))\n",
    "              for cell in row.find_all(\"td\", class_ = 'titleColumn')]\n",
    "                        for row in table1.find_all(\"tr\")]\n",
    "# tab_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>(La vida de los otros, 2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>(El retorno del Jedi, 1983)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>(Toro salvaje, 1980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>(Alien, el octavo pasajero: El montaje del dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>(Jurassic Park (Parque Jurásico), 1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>(Tres anuncios en las afueras, 2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>(El salario del miedo, 1953)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>(Una mente maravillosa, 2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>(Siempre a tu lado (Hachiko), 2009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>(Gladiator (El gladiador), 2000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "59                        (La vida de los otros, 2006)\n",
       "85                         (El retorno del Jedi, 1983)\n",
       "147                               (Toro salvaje, 1980)\n",
       "53   (Alien, el octavo pasajero: El montaje del dir...\n",
       "166            (Jurassic Park (Parque Jurásico), 1993)\n",
       "151               (Tres anuncios en las afueras, 2017)\n",
       "213                       (El salario del miedo, 1953)\n",
       "139                      (Una mente maravillosa, 2001)\n",
       "211                (Siempre a tu lado (Hachiko), 2009)\n",
       "42                    (Gladiator (El gladiador), 2000)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tab_data1)\n",
    "# randomly select rows from DF (10 items)\n",
    "df = df.sample(10) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting a list in a Pandas cell into multiple columns\n",
    "df = df[0].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Date release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>La vida de los otros</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>El retorno del Jedi</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>Toro salvaje</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>Alien, el octavo pasajero: El montaje del dire...</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>Jurassic Park (Parque Jurásico)</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>Tres anuncios en las afueras</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>El salario del miedo</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>Una mente maravillosa</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>Siempre a tu lado (Hachiko)</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>Gladiator (El gladiador)</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name Date release\n",
       "59                                La vida de los otros         2006\n",
       "85                                 El retorno del Jedi         1983\n",
       "147                                       Toro salvaje         1980\n",
       "53   Alien, el octavo pasajero: El montaje del dire...         1979\n",
       "166                    Jurassic Park (Parque Jurásico)         1993\n",
       "151                       Tres anuncios en las afueras         2017\n",
       "213                               El salario del miedo         1953\n",
       "139                              Una mente maravillosa         2001\n",
       "211                        Siempre a tu lado (Hachiko)         2009\n",
       "42                            Gladiator (El gladiador)         2000"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "df.columns = ['Name', 'Date release']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the city: Barcelona\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url15 = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "r = requests.get(url15)\n",
    "r.status_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coord': {'lon': 2.16, 'lat': 41.39}, 'weather': [{'id': 803, 'main': 'Clouds', 'description': 'broken clouds', 'icon': '04d'}], 'base': 'stations', 'main': {'temp': 26.24, 'feels_like': 28.82, 'temp_min': 25, 'temp_max': 27.22, 'pressure': 1011, 'humidity': 78}, 'visibility': 10000, 'wind': {'speed': 3.1, 'deg': 110}, 'clouds': {'all': 75}, 'dt': 1597601884, 'sys': {'type': 1, 'id': 6398, 'country': 'ES', 'sunrise': 1597554109, 'sunset': 1597603776}, 'timezone': 7200, 'id': 3128760, 'name': 'Barcelona', 'cod': 200}\n"
     ]
    }
   ],
   "source": [
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather in Barcelona\n",
      "Temperature: 26.24 C\n",
      "Wind speed: 3.1\n",
      "Description: Clouds\n",
      "Weather: broken clouds\n"
     ]
    }
   ],
   "source": [
    "# Extract data from .json\n",
    "name = r.json()['name']\n",
    "temp = r.json()['main']['temp']\n",
    "ws = r.json()['wind']['speed']\n",
    "descr = r.json()['weather'][0]['main']\n",
    "weath = r.json()['weather'][0]['description']\n",
    "\n",
    "# Results\n",
    "print(f'Weather in {name}')\n",
    "print(f'Temperature: {temp} C')\n",
    "print(f'Wind speed: {ws}')\n",
    "print(f'Description: {descr}')\n",
    "print(f'Weather: {weath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url16 = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "r = requests.get(url16)\n",
    "r.status_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISO-8859-1'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the encoding to eliminate the special character in price output\n",
    "r.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!--> <html lang=\"en-us\" class=\"no-js\"> <!--<![endif]-->\n",
      "    <head>\n",
      "        <title>\n",
      "    All products | Books to Scrape - Sandbox\n",
      "</title>\n",
      "\n",
      "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" /\n"
     ]
    }
   ],
   "source": [
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the tables in the page\n",
    "tables = soup.find_all(\"table\")\n",
    "# tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Light in the Attic'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name\n",
    "name = soup.find('article', class_ = 'product_pod').find('img')['alt']\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'£51.77'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# price\n",
    "price = soup.find('article', class_ = 'product_pod').find('div', class_ = 'product_price').find('p', class_ = 'price_color').text\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In stock'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stock availability\n",
    "stock = soup.find('article', class_ = 'product_pod').find('div', class_ = 'product_price').find('p', class_ = 'instock availability').text.replace('\\n','').strip()\n",
    "stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A Light in the Attic', '£51.77', 'In stock'),\n",
       " ('Tipping the Velvet', '£53.74', 'In stock'),\n",
       " ('Soumission', '£50.10', 'In stock'),\n",
       " ('Sharp Objects', '£47.82', 'In stock'),\n",
       " ('Sapiens: A Brief History of Humankind', '£54.23', 'In stock'),\n",
       " ('The Requiem Red', '£22.65', 'In stock'),\n",
       " ('The Dirty Little Secrets of Getting Your Dream Job', '£33.34', 'In stock'),\n",
       " ('The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull',\n",
       "  '£17.93',\n",
       "  'In stock'),\n",
       " ('The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics',\n",
       "  '£22.60',\n",
       "  'In stock'),\n",
       " ('The Black Maria', '£52.15', 'In stock'),\n",
       " ('Starving Hearts (Triangular Trade Trilogy, #1)', '£13.99', 'In stock'),\n",
       " (\"Shakespeare's Sonnets\", '£20.66', 'In stock'),\n",
       " ('Set Me Free', '£17.46', 'In stock'),\n",
       " (\"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\",\n",
       "  '£52.29',\n",
       "  'In stock'),\n",
       " ('Rip it Up and Start Again', '£35.02', 'In stock'),\n",
       " ('Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991',\n",
       "  '£57.25',\n",
       "  'In stock'),\n",
       " ('Olio', '£23.88', 'In stock'),\n",
       " ('Mesaerion: The Best Science Fiction Stories 1800-1849',\n",
       "  '£37.59',\n",
       "  'In stock'),\n",
       " ('Libertarianism for Beginners', '£51.33', 'In stock'),\n",
       " (\"It's Only the Himalayas\", '£45.17', 'In stock')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = [(book.find('img')['alt'], book.find('div', class_ = 'product_price').find('p', class_ = 'price_color').text, book.find('div', class_ = 'product_price').find('p', class_ = 'instock availability').text.replace('\\n','').strip()) for book in soup.find_all('article', class_ = 'product_pod')]\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name   Price  \\\n",
       "0                                A Light in the Attic  £51.77   \n",
       "1                                  Tipping the Velvet  £53.74   \n",
       "2                                          Soumission  £50.10   \n",
       "3                                       Sharp Objects  £47.82   \n",
       "4               Sapiens: A Brief History of Humankind  £54.23   \n",
       "5                                     The Requiem Red  £22.65   \n",
       "6   The Dirty Little Secrets of Getting Your Dream...  £33.34   \n",
       "7   The Coming Woman: A Novel Based on the Life of...  £17.93   \n",
       "8   The Boys in the Boat: Nine Americans and Their...  £22.60   \n",
       "9                                     The Black Maria  £52.15   \n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)  £13.99   \n",
       "11                              Shakespeare's Sonnets  £20.66   \n",
       "12                                        Set Me Free  £17.46   \n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...  £52.29   \n",
       "14                          Rip it Up and Start Again  £35.02   \n",
       "15  Our Band Could Be Your Life: Scenes from the A...  £57.25   \n",
       "16                                               Olio  £23.88   \n",
       "17  Mesaerion: The Best Science Fiction Stories 18...  £37.59   \n",
       "18                       Libertarianism for Beginners  £51.33   \n",
       "19                            It's Only the Himalayas  £45.17   \n",
       "\n",
       "   Stock availability  \n",
       "0            In stock  \n",
       "1            In stock  \n",
       "2            In stock  \n",
       "3            In stock  \n",
       "4            In stock  \n",
       "5            In stock  \n",
       "6            In stock  \n",
       "7            In stock  \n",
       "8            In stock  \n",
       "9            In stock  \n",
       "10           In stock  \n",
       "11           In stock  \n",
       "12           In stock  \n",
       "13           In stock  \n",
       "14           In stock  \n",
       "15           In stock  \n",
       "16           In stock  \n",
       "17           In stock  \n",
       "18           In stock  \n",
       "19           In stock  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(books)\n",
    "df.columns = ['Name', 'Price', 'Stock availability']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
